{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec介紹\n",
    "#Word2Vec其實是Word to Vector的簡稱，意在將每一個字轉換成一條向量，並讓這字的語意透過這條向量描繪出來\n",
    "#模型下載下來\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "\n",
    "pretrained_embeddings_path = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(pretrained_embeddings_path, \n",
    "binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#找到詞向量\n",
    "model['win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#找到相似字\n",
    "terms = ['taipei', 'queen', 'teacher', 'taiwan', 'dumpling']  ## 建立我們要查找的詞彙\n",
    "df = {}\n",
    "for t in terms:\n",
    "    if t in model.vocab:  ## 確認在訓練資療集當中是否有這個字，沒有這一步會出現錯誤\n",
    "        df[t] = [term for term, score in model.most_similar(t)]  ## 原本會回傳(term, score)的List，現在只抓term\n",
    "    else:\n",
    "        print(t, ' not in vocab')\n",
    "df = pd.DataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#產品標籤分類\n",
    "Import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "import string\n",
    "puns = string.punctuation\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "X = all_categories_vecs\n",
    "n_clusters= 30\n",
    "\n",
    "#Load Data\n",
    "with open('all_categories.list', 'r', encoding='utf8') as f:\n",
    "    all_categories = eval(f.read())\n",
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Model\n",
    "\n",
    "word_vec_mapping = {}\n",
    "path = \"word2vec.txt\"\n",
    "with open(path, 'r', encoding='utf8') as f:  ## 這個文檔的格式是一行一個字並配上他的向量，以空白鍵分隔\n",
    "    for line in f:  \n",
    "        tokens = line.split()\n",
    "        token = tokens[0]  ## 第一個token就是詞彙\n",
    "        vec = tokens[1:]  ## 後面的token向量\n",
    "        word_vec_mapping[token] = np.array(vec, dtype=np.float32)  ## 把整個model做成一個字典，以利查找字對應的向量\n",
    "        count += 1\n",
    "vec_dimensions = len(word_vec_mapping.get('men'))  ## 記錄這個mdoel每一個字的維度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#計算每一個category的向量\n",
    "#把每一個類別中每個字的向量相加做平均，就可以得到一個類別的向量了。如果是文章，長度不長的話可以考慮這個作法，但是嘗了之後效果就會變得很差。\n",
    "\n",
    "def doc2vec(doc, word2vec=word_vec_mapping):\n",
    "    docvec=np.zeros(vec_dimensions, )  ## 先處使劃一條向量，如果某個類別裡面的字都沒有在字典裡，那麼會回傳這條向量\n",
    "    vec_count = 1\n",
    "    \n",
    "    if pd.notnull(doc):\n",
    "        terms = tokenize(doc)  ## 把類別tokenize成一個個的詞彙\n",
    "        for term in terms:\n",
    "            termvec = word_vec_mapping.get(term, None)  ## 得到詞向量\n",
    "            if termvec is not None:\n",
    "                docvec += np.array(termvec, dtype=np.float32)  ## 把詞向量家道類別向量中\n",
    "                vec_count += 1              \n",
    "    return (docvec/vec_count)  ##  記得加了幾條向量，就要處以相應的數字取平均\n",
    "\n",
    "all_categories_vecs = np.concatenate((pd.Series(all_categories).apply(doc2vec).values)).reshape(len(all_categories), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kmean 分群\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "all_categories_labels_kmeans = kmeans.fit_predict(X)\n",
    "df_cat = pd.DataFrame(all_categories_labels_kmeans, index=all_categories, columns=['label'])\n",
    "for i in range(len(set(all_categories_labels_kmeans))):\n",
    "    cats = list(df_cat[df_cat['label'] == i].index)    \n",
    "    print(\"cluster \" + str(i) + \": \")\n",
    "    print(list(cats))\n",
    "    print(\"=============================================\")\n",
    "    print(\"=============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用DBSCAN尋找outlier\n",
    "\n",
    "all_categories_labels_dbscam = DBSCAN().fit_predict(X)\n",
    "Counter(all_categories_labels_dbscam)\n",
    "df_cat = pd.DataFrame(all_categories_labels_dbscam, index=all_categories, columns=['label'])\n",
    "print(list(df_cat[df_cat['label'] == 0].index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
